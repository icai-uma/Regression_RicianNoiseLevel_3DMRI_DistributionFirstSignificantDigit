{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iqpaZxx1kEE4"
      },
      "source": [
        "# Benford for 3D MRI - Model training with cross validation\n",
        "\n",
        "This script trains several machine learning regression models to estimate the level of Rician noise from the divergences of the observed probability distributions of the first and second digits of the transformed versions of noisy 3D magnetic resonance images (MRIs) with respect to the theoretical Benford probability distributions of those digits.\n",
        "\n",
        "We focus on the distributions of the first digits because the distributions of the second digits do not seem to contain much useful information.\n",
        "\n",
        "In order to keep things simple, we have tried all sets of input features for the regression models with one and two input features.\n",
        "\n",
        "We apply 10-fold cross validation. Samples from the same image are strongly correlated so they cannot be used in the training and validation sets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "%reset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "uKGNAzhXkBQ3"
      },
      "outputs": [],
      "source": [
        "# Import the relevant libraries\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import scipy\n",
        "from scipy import stats\n",
        "import os\n",
        "from glob import glob \n",
        "import pickle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "gg2oCZ3y3fQq"
      },
      "outputs": [],
      "source": [
        "# Define the dataset to use, choosing one of the directories: 20OASIS_SynapseWeb_brain, OASIS-TRT-20_volumes, 12HLN, NKI-TRT-20_volumes, NKI-RS-22_volumes, MMRR-21_volumes\n",
        "repository = '12HLN' # MUST TO DEFINE: #20OASIS_SynapseWeb_brain #OASIS-TRT-20_volumes #12HLN #NKI-TRT-20_volumes #NKI-RS-22_volumes #MMRR-21_volumes\n",
        "typeImage = 't1weighted.nii.gz' #'t1weighted_brain.nii.gz' #'t1weighted.nii.gz'\n",
        "inputDir = ('../input') \n",
        "outputDir = ('../output')\n",
        "\n",
        "# Path with input images\n",
        "dataMRI = os.path.join(inputDir, repository)\n",
        "pathMRI = sorted(glob(dataMRI + '/**/' + typeImage, recursive=True))\n",
        "SIZE = len(pathMRI)\n",
        "\n",
        "# Training dataset path\n",
        "str_path = os.path.join(outputDir + '/input/' + repository + '_head.pkl') \n",
        "dataset_path = str_path.replace(\"_volumes\", \"\")\n",
        "\n",
        "# Result folder path\n",
        "str_path = os.path.join(outputDir + '/Figures_and_tables/' + repository)\n",
        "result_path = str_path.replace(\"volumes\", \"head\")\n",
        "\n",
        "# Experiment parameters\n",
        "NumNoiseLevels=20\n",
        "NumNoiselessImages=SIZE # SIZE is the number of images in the repository: HLN=12; MMRR=21; NKI-RS=22; NKI-TRT=20; OASIS-TRT=20\n",
        "MaxNoiseLevel=0.4\n",
        "TestedTransforms=[\"FFT\",\"DCT\",\"DST\"]\n",
        "TestedPerformanceMeasures=[\"MSE\",\"MAE\",\"R2\"]\n",
        "TestedRegressors=[\"Linear\",\"Poly\",\"RandomForest\",\"SVR\",\"KernelRegression\"]\n",
        "NumSplits=10"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "WiP0VPHf3TiL"
      },
      "outputs": [],
      "source": [
        "# Load the training data file\n",
        "with open(dataset_path,'rb') as MyFile:\n",
        "  [NoiseLevels,Divergences]=pickle.load(MyFile)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "LWCAo9rA44Oz",
        "outputId": "e9488bff-1d56-44f5-ca35-cd4b0cc87774"
      },
      "outputs": [],
      "source": [
        "# Import machine learning regression models\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.svm import SVR\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from statsmodels.nonparametric.kernel_regression import KernelReg\n",
        "from sklearn.metrics import r2_score\n",
        "\n",
        "# Import cross validation\n",
        "from sklearn.model_selection import KFold\n",
        "\n",
        "# Import linear algebra error\n",
        "from numpy.linalg import LinAlgError\n",
        "\n",
        "# Define the input features that will be considered. Each input feature is defined\n",
        "# by a tuple that indicates the transform, the probability distribution divergence\n",
        "# measure, and the digit (first or second)\n",
        "InputFeatures=[(\"FFT\",\"BD\",\"First\"),(\"DCT\",\"BD\",\"First\"),(\"DST\",\"BD\",\"First\"),\n",
        "               (\"FFT\",\"KL\",\"First\"),(\"DCT\",\"KL\",\"First\"),(\"DST\",\"KL\",\"First\"),\n",
        "               (\"FFT\",\"TV\",\"First\"),(\"DCT\",\"TV\",\"First\"),(\"DST\",\"TV\",\"First\"),\n",
        "               (\"FFT\",\"H\",\"First\"),(\"DCT\",\"H\",\"First\"),(\"DST\",\"H\",\"First\"),\n",
        "               (\"FFT\",\"JS\",\"First\"),(\"DCT\",\"JS\",\"First\"),(\"DST\",\"JS\",\"First\"),\n",
        "               (\"FFT\",\"BD\",\"Second\"),(\"DCT\",\"BD\",\"Second\"),(\"DST\",\"BD\",\"Second\"),\n",
        "               (\"FFT\",\"KL\",\"Second\"),(\"DCT\",\"KL\",\"Second\"),(\"DST\",\"KL\",\"Second\"),\n",
        "               (\"FFT\",\"TV\",\"Second\"),(\"DCT\",\"TV\",\"Second\"),(\"DST\",\"TV\",\"Second\"),\n",
        "               (\"FFT\",\"H\",\"Second\"),(\"DCT\",\"H\",\"Second\"),(\"DST\",\"H\",\"Second\"),\n",
        "               (\"FFT\",\"JS\",\"Second\"),(\"DCT\",\"JS\",\"Second\"),(\"DST\",\"JS\",\"Second\")\n",
        "               ]\n",
        "\n",
        "# Define the sets of model input features to be tried. Each set is defined by \n",
        "# a list of input features which are indices into the InputFeatures list.\n",
        "# The sets [0] and [3] were presented in the MICCAI paper. All the others are new.\n",
        "CountInputFeatures=15\n",
        "SetsInputFeatures=[]\n",
        "for NdxFeature in range(0,CountInputFeatures):\n",
        "  SetsInputFeatures.append([NdxFeature])\n",
        "for NdxFeature in range(0,CountInputFeatures):\n",
        "  for NdxFeature2 in range(NdxFeature+1,CountInputFeatures):\n",
        "    SetsInputFeatures.append([NdxFeature,NdxFeature2])\n",
        "# These additional sets of features perform fairly well\n",
        "SetsInputFeatures.append([3,10,13])\n",
        "SetsInputFeatures.append([3,4,5])\n",
        "SetsInputFeatures.append([3,4,5,8])\n",
        "# The number of sets of inputs features, to be used afterwards\n",
        "NumSets=len(SetsInputFeatures)\n",
        "\n",
        "# Create a name for each set of input features. Each name is a string with the input\n",
        "# feature indices separated by blank spaces\n",
        "SetNames=[]\n",
        "for NdxSet in range(0,NumSets):\n",
        "  SetNames.append(\" \".join(map(str,SetsInputFeatures[NdxSet])))\n",
        "\n",
        "# Create the cross validation class to obtain the image indices for the training\n",
        "# and validation sets\n",
        "CrossValidator = KFold(n_splits=NumSplits, shuffle=True, random_state=1)\n",
        "\n",
        "# Prepare the validation results output variable\n",
        "ValidationResults={}\n",
        "\n",
        "for NdxFold, (TrainImagesIndices, ValidationImagesIndices) in enumerate(CrossValidator.split(np.arange(NumNoiselessImages))):\n",
        "  print(f\"Fold {NdxFold}:\")\n",
        "  print(f\"  Train: index={TrainImagesIndices}\")\n",
        "  print(f\"  Test:  index={ValidationImagesIndices}\")\n",
        "\n",
        "  # Obtain the training noise levels (outputs to be predicted by the regressors) from\n",
        "  # the training noiseless images \n",
        "  TrainingNoiseLevels=NoiseLevels[TrainImagesIndices,:]\n",
        "  # Reshape to the (n_samples, n_targets) format required by\n",
        "  # the sklearn.linear_model.LinearRegression class\n",
        "  TrainingNoiseLevelsFlat=np.reshape(TrainingNoiseLevels,(len(TrainImagesIndices)*NumNoiseLevels,1))\n",
        "\n",
        "  # Obtain the validation noise levels (outputs to be predicted by the regressors) from\n",
        "  # the validation noiseless images\n",
        "  ValidationNoiseLevels=NoiseLevels[ValidationImagesIndices,:]\n",
        "  # Reshape to the (n_samples, n_targets) format required by\n",
        "  # the sklearn.linear_model.LinearRegression class\n",
        "  ValidationNoiseLevelsFlat=np.reshape(ValidationNoiseLevels,(len(ValidationImagesIndices)*NumNoiseLevels,1))\n",
        "\n",
        "\n",
        "\n",
        "  # Train the models and validate their performance\n",
        "  for NdxSet in range(0,NumSets):\n",
        "\n",
        "    # Print the variable indices for the current model\n",
        "    print(SetsInputFeatures[NdxSet])\n",
        "\n",
        "    # Obtain the number of input features (distribution distances) for the current model\n",
        "    NumInputFeatures=len(SetsInputFeatures[NdxSet])\n",
        "    \n",
        "    # Obtain the input features (distribution distances) for the training and validation sets\n",
        "    TrainingDistributionDistances=np.zeros((NumInputFeatures,len(TrainImagesIndices),NumNoiseLevels))\n",
        "    ValidationDistributionDistances=np.zeros((NumInputFeatures,len(ValidationImagesIndices),NumNoiseLevels))\n",
        "    # Loop for all input features in the current set\n",
        "    for NdxInputFeature in range(0,NumInputFeatures):\n",
        "      # Fetch the input features for all noiseless images except the validation noiseless image\n",
        "      TrainingDistributionDistances[NdxInputFeature,:,:]=Divergences[InputFeatures[SetsInputFeatures[NdxSet][NdxInputFeature]]][TrainImagesIndices,:]\n",
        "      # Fetch the input features for the validation noiseless image\n",
        "      ValidationDistributionDistances[NdxInputFeature,:,:]=Divergences[InputFeatures[SetsInputFeatures[NdxSet][NdxInputFeature]]][ValidationImagesIndices,:]\n",
        "\n",
        "    # Reshape the input features to the format (n_samples, n_features) which is required\n",
        "    # by the sklearn.linear_model.LinearRegression class\n",
        "    TrainingDistributionDistancesFlat=np.reshape(TrainingDistributionDistances,(NumInputFeatures,len(TrainImagesIndices)*NumNoiseLevels)).transpose()\n",
        "    ValidationDistributionDistancesFlat=np.reshape(ValidationDistributionDistances,(NumInputFeatures,len(ValidationImagesIndices)*NumNoiseLevels)).transpose()\n",
        "\n",
        "    # Train the linear regression model\n",
        "    LinearModel1D=LinearRegression().fit(TrainingDistributionDistancesFlat,TrainingNoiseLevelsFlat)\n",
        "\n",
        "    # Train the polynomial regression model\n",
        "    poly = PolynomialFeatures(degree=2, include_bias=False)\n",
        "    TrainingPolyFeatures = poly.fit_transform(TrainingDistributionDistancesFlat)\n",
        "    PolyModel1D = LinearRegression()\n",
        "    PolyModel1D.fit(TrainingPolyFeatures, TrainingNoiseLevelsFlat)\n",
        "\n",
        "    # Train the random forest regression model\n",
        "    RandomForestModel=RandomForestRegressor(min_samples_leaf=20).fit(TrainingDistributionDistancesFlat,TrainingNoiseLevelsFlat.ravel())\n",
        "\n",
        "    # Train the support vector regression model\n",
        "    SVRModel = make_pipeline(StandardScaler(), SVR(C=1.0, epsilon=0.001)).fit(TrainingDistributionDistancesFlat,TrainingNoiseLevelsFlat.ravel())\n",
        "\n",
        "    # Train the kernel regression model\n",
        "    try:\n",
        "      KRModel = KernelReg(endog=TrainingNoiseLevelsFlat.ravel(), exog=TrainingDistributionDistancesFlat, var_type=\"c\"*NumInputFeatures, bw=\"cv_ls\") \n",
        "    except LinAlgError:\n",
        "      print(\"Linear algebra error while training the kernel regressor\")\n",
        "\n",
        "    # Validate the linear regression model\n",
        "    ValidationNoiseLevelsLinear=LinearModel1D.predict(ValidationDistributionDistancesFlat)\n",
        "    ValidationResults[NdxFold,NdxSet,\"MSE\",\"Linear\"]=np.mean(np.power(ValidationNoiseLevelsLinear-ValidationNoiseLevelsFlat,2))\n",
        "    ValidationResults[NdxFold,NdxSet,\"MAE\",\"Linear\"]=np.mean(np.absolute(ValidationNoiseLevelsLinear-ValidationNoiseLevelsFlat))\n",
        "    ValidationResults[NdxFold,NdxSet,\"R2\",\"Linear\"]=r2_score(ValidationNoiseLevelsFlat,ValidationNoiseLevelsLinear)\n",
        "\n",
        "    # Validate the polynomial regression model\n",
        "    ValidationPolyFeatures = poly.fit_transform(ValidationDistributionDistancesFlat)\n",
        "    ValidationNoiseLevelsPoly=PolyModel1D.predict(ValidationPolyFeatures)\n",
        "    ValidationResults[NdxFold,NdxSet,\"MSE\",\"Poly\"]=np.mean(np.power(ValidationNoiseLevelsPoly-ValidationNoiseLevelsFlat,2))\n",
        "    ValidationResults[NdxFold,NdxSet,\"MAE\",\"Poly\"]=np.mean(np.absolute(ValidationNoiseLevelsPoly-ValidationNoiseLevelsFlat))\n",
        "    ValidationResults[NdxFold,NdxSet,\"R2\",\"Poly\"]=r2_score(ValidationNoiseLevelsFlat,ValidationNoiseLevelsPoly)  \n",
        "\n",
        "    # Validate the random forest regression model\n",
        "    ValidationNoiseLevelsRandomForest=np.reshape(RandomForestModel.predict(ValidationDistributionDistancesFlat),(NumNoiseLevels*len(ValidationImagesIndices),1))\n",
        "    ValidationResults[NdxFold,NdxSet,\"MSE\",\"RandomForest\"]=np.mean(np.power(ValidationNoiseLevelsRandomForest-ValidationNoiseLevelsFlat,2))\n",
        "    ValidationResults[NdxFold,NdxSet,\"MAE\",\"RandomForest\"]=np.mean(np.absolute(ValidationNoiseLevelsRandomForest-ValidationNoiseLevelsFlat))\n",
        "    ValidationResults[NdxFold,NdxSet,\"R2\",\"RandomForest\"]=r2_score(ValidationNoiseLevelsFlat,ValidationNoiseLevelsRandomForest)  \n",
        "    \n",
        "    # Validate the support vector regression model\n",
        "    ValidationNoiseLevelsSVR=np.reshape(SVRModel.predict(ValidationDistributionDistancesFlat),(NumNoiseLevels*len(ValidationImagesIndices),1))\n",
        "    ValidationResults[NdxFold,NdxSet,\"MSE\",\"SVR\"]=np.mean(np.power(ValidationNoiseLevelsSVR-ValidationNoiseLevelsFlat,2))\n",
        "    ValidationResults[NdxFold,NdxSet,\"MAE\",\"SVR\"]=np.mean(np.absolute(ValidationNoiseLevelsSVR-ValidationNoiseLevelsFlat))\n",
        "    ValidationResults[NdxFold,NdxSet,\"R2\",\"SVR\"]=r2_score(ValidationNoiseLevelsFlat,ValidationNoiseLevelsSVR)  \n",
        "\n",
        "    # Validate the kernel regression model\n",
        "    # Error to estimate hyperparameter\n",
        "    try:\n",
        "      (Averages,StandardDeviations)=KRModel.fit(ValidationDistributionDistancesFlat)\n",
        "      ValidationNoiseLevelsKR=np.reshape(Averages,(NumNoiseLevels*len(ValidationImagesIndices),1))\n",
        "      ValidationResults[NdxFold,NdxSet,\"R2\",\"KernelRegression\"]=r2_score(ValidationNoiseLevelsFlat,ValidationNoiseLevelsKR)  \n",
        "      ValidationResults[NdxFold,NdxSet,\"MSE\",\"KernelRegression\"]=np.mean(np.power(ValidationNoiseLevelsKR-ValidationNoiseLevelsFlat,2))\n",
        "      ValidationResults[NdxFold,NdxSet,\"MAE\",\"KernelRegression\"]=np.mean(np.absolute(ValidationNoiseLevelsKR-ValidationNoiseLevelsFlat))\n",
        "    except LinAlgError: # Error del 21MMRR, fallo en entrenamiento, continua y hae el fit. Pero falla en reshape.\n",
        "      print(\"Linear algebra error while training the kernel regressor so can not do reshape after fit.\")\n",
        "      ValidationResults[NdxFold,NdxSet,\"R2\",\"KernelRegression\"]=r2_score(ValidationNoiseLevelsFlat,ValidationNoiseLevelsKR)   \n",
        "      ValidationResults[NdxFold,NdxSet,\"MSE\",\"KernelRegression\"]=np.mean(np.power(ValidationNoiseLevelsKR-ValidationNoiseLevelsFlat,2))\n",
        "      ValidationResults[NdxFold,NdxSet,\"MAE\",\"KernelRegression\"]=np.mean(np.absolute(ValidationNoiseLevelsKR-ValidationNoiseLevelsFlat))\n",
        "    except ValueError: # Error del 22NKI, Nan en Average y da error en R2.\n",
        "      print('Nan or Inf value in R2 because of to KernelModel small bandwith')\n",
        "      ValidationResults[NdxFold,NdxSet,\"MSE\",\"KernelRegression\"]=np.Inf\n",
        "      ValidationResults[NdxFold,NdxSet,\"MAE\",\"KernelRegression\"]=np.Inf\n",
        "      ValidationResults[NdxFold,NdxSet,\"R2\",\"KernelRegression\"]=0\n",
        "    \n",
        "    # Plot the regression results for the models with only one input feature\n",
        "    if NumInputFeatures==1:\n",
        "      # Sort the values of the independent variable for the plot (the distribution divergence values)\n",
        "      SortedIndices = np.argsort(ValidationDistributionDistancesFlat.flatten())\n",
        "      fig=plt.figure()\n",
        "      plt.plot(TrainingDistributionDistancesFlat, TrainingNoiseLevelsFlat, 'o', ValidationDistributionDistancesFlat[SortedIndices], ValidationNoiseLevelsLinear[SortedIndices], '-',\n",
        "              ValidationDistributionDistancesFlat[SortedIndices],ValidationNoiseLevelsPoly[SortedIndices],'-',ValidationDistributionDistancesFlat[SortedIndices],ValidationNoiseLevelsRandomForest[SortedIndices],'-',\n",
        "              ValidationDistributionDistancesFlat[SortedIndices],ValidationNoiseLevelsSVR[SortedIndices],'-',ValidationDistributionDistancesFlat[SortedIndices],ValidationNoiseLevelsKR[SortedIndices],'-', markersize=1.3)\n",
        "      plt.legend(['data', 'linear', 'poly', 'RF', 'SVR', 'KR'], loc='best')\n",
        "      plt.xlabel('Mi') # MUST TO DEFINE: write the value of the metric with a number.\n",
        "      plt.ylabel('Noise level')\n",
        "      plot_path = os.path.join(result_path + '/Fold%s_Plot%s.pdf' % (NdxFold, SetNames[NdxSet]))\n",
        "      plt.savefig(plot_path, format=\"pdf\", bbox_inches=\"tight\")\n",
        "      #plt.show()\n",
        "      plt.close(fig)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 779
        },
        "id": "LvIEPGzhePcx",
        "outputId": "74b8283d-3ba8-4c8c-b1db-ca3eb38c04a4"
      },
      "outputs": [],
      "source": [
        "# Write the results to an Excel spreadsheet file\n",
        "\n",
        "# Average the validation results across the splits\n",
        "ValidationResultsAverages={}\n",
        "for NdxSet in range(0,NumSets):\n",
        "  for MyPerformanceMeasure in TestedPerformanceMeasures:\n",
        "    for MyRegressor in TestedRegressors:\n",
        "      MyResults=np.zeros((NumSplits,1))\n",
        "      for NdxFold in np.arange(NumSplits):\n",
        "        MyResults[NdxFold]=ValidationResults[NdxFold,NdxSet,MyPerformanceMeasure,MyRegressor]\n",
        "      ValidationResultsAverages[NdxSet,MyPerformanceMeasure,MyRegressor]=np.mean(MyResults)\n",
        "\n",
        "# Import the library to write Excel spreadsheet files\n",
        "import xlwt\n",
        "\n",
        "# Define the performance measures and the regressors\n",
        "HigherBetterPerformance={\"MSE\":False,\"MAE\":False,\"R2\":True}\n",
        "TestedRegressorsSmall=[\"Linear\",\"Poly\",\"RF\",\"SVR\",\"KR\"]\n",
        "\n",
        "# Text style with boldface for the best configuration\n",
        "StyleBestConfig = xlwt.easyxf('font: bold on')\n",
        "\n",
        "# Create the workbook\n",
        "MyWorkBook = xlwt.Workbook()\n",
        "\n",
        "# Loop for all the tested performance measures (one per worksheet in the Excel spreadsheet file)\n",
        "for MyPerformanceMeasure in TestedPerformanceMeasures:\n",
        "\n",
        "  # Add a sheet for the current performance measure\n",
        "  MyWorkSheet = MyWorkBook.add_sheet(MyPerformanceMeasure)\n",
        "\n",
        "  # Create an array with the performance values for all sets and regressors\n",
        "  MyPerformances=np.zeros((NumSets,len(TestedRegressors)))\n",
        "  for NdxRegressor in range(0,len(TestedRegressors)):\n",
        "    MyRegressor=TestedRegressors[NdxRegressor]\n",
        "    # Obtain the performance values for all sets of input features\n",
        "    for NdxSet in range(0,NumSets):\n",
        "      MyPerformances[NdxSet,NdxRegressor]=ValidationResultsAverages[NdxSet,MyPerformanceMeasure,MyRegressor]\n",
        "\n",
        "  # Obtain the indices of the best performing configuration (set and regressor)\n",
        "  # for this performance measure\n",
        "  if HigherBetterPerformance[MyPerformanceMeasure]:\n",
        "    # Higher is better\n",
        "    IndexBestConfig = np.unravel_index(np.argmax(MyPerformances, axis=None), (NumSets,len(TestedRegressors)))\n",
        "  else:\n",
        "    # Lower is better\n",
        "    IndexBestConfig = np.unravel_index(np.argmin(MyPerformances, axis=None), (NumSets,len(TestedRegressors)))\n",
        "\n",
        "  # Sort the sets by their performance. The performance of a set is the performance of the\n",
        "  # best regressor for that set\n",
        "  if HigherBetterPerformance[MyPerformanceMeasure]:\n",
        "    # Higher is better\n",
        "    BestRegressorPerformances=np.amax(MyPerformances, axis=1)\n",
        "    RankedSetIndices=np.argsort(-BestRegressorPerformances)\n",
        "  else:\n",
        "    # Lower is better\n",
        "    BestRegressorPerformances=np.amin(MyPerformances, axis=1)\n",
        "    RankedSetIndices=np.argsort(BestRegressorPerformances)\n",
        "\n",
        "  # Show a heatmap with the performance values\n",
        "  HeatMapValues=np.zeros((CountInputFeatures,CountInputFeatures))\n",
        "  # Index to traverse the BestRegressorPerformances array\n",
        "  NdxThisFeature=0\n",
        "  # Process all feature sets with a single feature\n",
        "  for NdxFeature in range(0,CountInputFeatures):\n",
        "    HeatMapValues[NdxFeature,NdxFeature]=BestRegressorPerformances[NdxThisFeature]\n",
        "    NdxThisFeature=NdxThisFeature+1\n",
        "  # Process all feature sets with two features\n",
        "  for NdxFeature in range(0,CountInputFeatures):\n",
        "    for NdxFeature2 in range(NdxFeature+1,CountInputFeatures):\n",
        "      HeatMapValues[NdxFeature,NdxFeature2]=BestRegressorPerformances[NdxThisFeature]\n",
        "      HeatMapValues[NdxFeature2,NdxFeature]=HeatMapValues[NdxFeature,NdxFeature2]\n",
        "      NdxThisFeature=NdxThisFeature+1\n",
        "  # MyPerformances[20,4]=np.nan # The KR for the 3 4 5 8 set works badly for bw=\"aic\"\n",
        "  MyFigure, MyAxes = plt.subplots()\n",
        "  MyImage = MyAxes.imshow(HeatMapValues)\n",
        "  # Show all ticks and label them with the respective list entries\n",
        "  MyAxes.set_yticks([0,5,10,14])\n",
        "  MyAxes.set_yticklabels([\"0\",\"5\",\"10\",\"14\"])\n",
        "  MyAxes.set_xticks([0,5,10,14])\n",
        "  MyAxes.set_xticklabels([\"0\",\"5\",\"10\",\"14\"])\n",
        "  # Rotate the tick labels and set their alignment\n",
        "  plt.setp(MyAxes.get_xticklabels(), rotation=45, ha=\"right\", rotation_mode=\"anchor\")\n",
        "  heatmatp_path = os.path.join(result_path + '/%s.pdf' % MyPerformanceMeasure)\n",
        "  plt.savefig(heatmatp_path, format=\"pdf\", bbox_inches=\"tight\")\n",
        "  plt.show()\n",
        "\n",
        "\n",
        "  # Generate a table with the performance values, sorted by the set indices\n",
        "  # Write the first row which lists the regressors\n",
        "  for NdxRegressor in range(0,len(TestedRegressors)):\n",
        "    MyRegressor=TestedRegressors[NdxRegressor]\n",
        "    MyWorkSheet.write(0,NdxRegressor+1,MyRegressor)\n",
        "  # Loop for all sets (one set per row in the worksheet)\n",
        "  for NdxSet in range(0,NumSets):\n",
        "    # Write the model identification\n",
        "    MyWorkSheet.write(NdxSet+1,0,\" \".join(map(str,SetsInputFeatures[NdxSet])))\n",
        "    # Loop for all regressors (one regressor per column in the worksheet)\n",
        "    for NdxRegressor in range(0,len(TestedRegressors)):\n",
        "      MyRegressor=TestedRegressors[NdxRegressor]\n",
        "      # Write the performance value. Apply boldface in case that it is the best configuration\n",
        "      if (IndexBestConfig[0]==NdxSet) and (IndexBestConfig[1]==NdxRegressor):\n",
        "        MyWorkSheet.write(NdxSet+1,NdxRegressor+1,ValidationResultsAverages[NdxSet,MyPerformanceMeasure,MyRegressor],StyleBestConfig)\n",
        "      else:\n",
        "        MyWorkSheet.write(NdxSet+1,NdxRegressor+1,ValidationResultsAverages[NdxSet,MyPerformanceMeasure,MyRegressor])\n",
        "\n",
        "  # Generate a table with the performance values, sorted by the performance of the sets\n",
        "  # Write the first row which lists the regressors\n",
        "  for NdxRegressor in range(0,len(TestedRegressors)):\n",
        "    MyRegressor=TestedRegressors[NdxRegressor]\n",
        "    MyWorkSheet.write(0,NdxRegressor+len(TestedRegressors)+3,MyRegressor)\n",
        "  # Loop for all sets (one set per row in the worksheet)\n",
        "  for NdxSet in range(0,NumSets):\n",
        "    # The index of the NdxSet-th best set according to their performance\n",
        "    NdxMyRankedSet=RankedSetIndices[NdxSet]\n",
        "    # Write the model identification\n",
        "    MyWorkSheet.write(NdxSet+1,len(TestedRegressors)+2,\" \".join(map(str,SetsInputFeatures[NdxMyRankedSet])))\n",
        "    # Loop for all regressors (one regressor per column in the worksheet)\n",
        "    for NdxRegressor in range(0,len(TestedRegressors)):\n",
        "      MyRegressor=TestedRegressors[NdxRegressor]\n",
        "      # Write the performance value. Apply boldface in case that it is the best configuration\n",
        "      if (IndexBestConfig[0]==NdxMyRankedSet) and (IndexBestConfig[1]==NdxRegressor):\n",
        "        MyWorkSheet.write(NdxSet+1,NdxRegressor+len(TestedRegressors)+3,ValidationResultsAverages[NdxMyRankedSet,MyPerformanceMeasure,MyRegressor],StyleBestConfig)\n",
        "      else:\n",
        "        MyWorkSheet.write(NdxSet+1,NdxRegressor+len(TestedRegressors)+3,ValidationResultsAverages[NdxMyRankedSet,MyPerformanceMeasure,MyRegressor])\n",
        "  \n",
        "# Save the Excel spreadsheet file\n",
        "excel_path = os.path.join(result_path + '/CrossValidationResults.xls')\n",
        "MyWorkBook.save(excel_path)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "envLiver",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10 (default, Jun  4 2021, 15:09:15) \n[GCC 7.5.0]"
    },
    "vscode": {
      "interpreter": {
        "hash": "dd6b9c4b4ca2845646bb4eab80d4bb641d150353eb671a54a91d5efad71f9eee"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
