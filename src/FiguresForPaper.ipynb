{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "iqpaZxx1kEE4"
      },
      "source": [
        "# Benford for 3D MRI - Model training with cross validation - Plot Figures with 2 metrics as input\n",
        "\n",
        "This script trains several machine learning regression models to estimate the level of Rician noise from the divergences of the observed probability distributions of the first and second digits of the transformed versions of noisy 3D magnetic resonance images (MRIs) with respect to the theoretical Benford probability distributions of those digits.\n",
        "\n",
        "We focus on the distributions of the first digits because the distributions of the second digits do not seem to contain much useful information.\n",
        "\n",
        "We have tried all sets of input features for the regression models with two input features. \n",
        "\n",
        "We apply 10-fold cross validation. Samples from the same image are strongly correlated so they cannot be used in the training and validation sets.\n",
        "\n",
        "Finally figures are plotted."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [],
      "source": [
        "%reset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "uKGNAzhXkBQ3"
      },
      "outputs": [],
      "source": [
        "# Import the relevant libraries\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import scipy\n",
        "from scipy import stats\n",
        "import os \n",
        "from glob import glob \n",
        "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
        "import pickle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "gg2oCZ3y3fQq"
      },
      "outputs": [],
      "source": [
        "# Define the dataset to use, choosing one of the directories: 20OASIS_SynapseWeb_brain, OASIS-TRT-20_volumes, 12HLN, NKI-TRT-20_volumes, NKI-RS-22_volumes, MMRR-21_volumes\n",
        "repository = '12HLN' # MUST TO DEFINE: #20OASIS_SynapseWeb_brain #OASIS-TRT-20_volumes #12HLN #NKI-TRT-20_volumes #NKI-RS-22_volumes #MMRR-21_volumes\n",
        "typeImage = 't1weighted.nii.gz' #'t1weighted_brain.nii.gz' #'t1weighted.nii.gz'\n",
        "inputDir = ('../input') \n",
        "outputDir = ('../output')\n",
        "outputInputDir = outputDir + '/input/'\n",
        "result_path = os.path.join(outputDir, repository)\n",
        "\n",
        "# Path with input images\n",
        "dataMRI = os.path.join(inputDir, repository)\n",
        "pathMRI = sorted(glob(dataMRI + '/**/' + typeImage, recursive=True))\n",
        "SIZE = len(pathMRI)\n",
        "\n",
        "extension = '.pkl'\n",
        "outputFile = repository + extension\n",
        "dataset_path = os.path.join(outputInputDir, outputFile)\n",
        "\n",
        "# Experiment parameters\n",
        "NumNoiseLevels=20\n",
        "NumNoiselessImages=SIZE # SIZE is the number of images in the repository: HLN=12; MMRR=21; NKI-RS=22; NKI-TRT=20; OASIS-TRT=20\n",
        "MaxNoiseLevel=0.4\n",
        "TestedTransforms=[\"FFT\",\"DCT\",\"DST\"]\n",
        "TestedPerformanceMeasures=[\"MSE\",\"MAE\",\"R2\"]\n",
        "TestedRegressors=[\"Linear\",\"Poly\",\"RandomForest\",\"SVR\",\"KernelRegression\"]\n",
        "NumSplits=10"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "WiP0VPHf3TiL"
      },
      "outputs": [],
      "source": [
        "# Load the training data file\n",
        "with open(dataset_path,'rb') as MyFile: \n",
        "  [NoiseLevels,Divergences]=pickle.load(MyFile)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Plot the best variables set and model (choose by first row in excel file) -\n",
        "\n",
        "def plot_3D(xValidation, yValidation, noiseValidationPredicted, noiseValidationGT, xTraining, yTraining, noiseTrainingGT, xlabel, ylabel):\n",
        "       \n",
        "       fig, ax = plt.subplots(figsize=(10, 5))\n",
        "       ax = plt.gca()\n",
        "       \n",
        "       sc1 = plt.scatter(xTraining, yTraining,\n",
        "              marker=\"o\", \n",
        "              cmap='Blues',\n",
        "              linewidths=1, alpha=.7,\n",
        "              edgecolor='k',\n",
        "              s = 30,\n",
        "              c=noiseTrainingGT, \n",
        "              vmin=0.00, vmax=0.50)\n",
        "       sc2 = plt.scatter(xValidation, yValidation,\n",
        "              marker=\"^\",\n",
        "              cmap='Reds',\n",
        "              linewidths=1, alpha=.7,\n",
        "              edgecolor='k',\n",
        "              s = 200,\n",
        "              c=noiseValidationPredicted, \n",
        "              vmin=0.00, vmax=0.50)\n",
        "       sc3 = plt.scatter(xValidation, yValidation,\n",
        "              marker=\"s\", \n",
        "              cmap='Greens',\n",
        "              linewidths=1, alpha=.7,\n",
        "              edgecolor='k',\n",
        "              s = 50,\n",
        "              c=noiseValidationGT, \n",
        "              vmin=0.00, vmax=0.50)\n",
        "       \n",
        "       plt.xlabel('M' + xlabel, fontsize=10)\n",
        "       plt.ylabel('M' + ylabel, fontsize=10)\n",
        "       plt.title('')\n",
        "       plt.tick_params(axis='both', labelsize=10)\n",
        "       \n",
        "       divider = make_axes_locatable(ax)\n",
        "       cax1 = divider.append_axes(\"right\", size=\"5%\", pad=0.20)\n",
        "       cax2 = divider.append_axes(\"right\", size=\"5%\", pad=0.75)\n",
        "       cax3 = divider.append_axes(\"right\", size=\"5%\", pad=0.80)\n",
        "\n",
        "       cabar1 = plt.colorbar(sc1, cax=cax1)\n",
        "       cabar1.set_label('Training data, GT noise')\n",
        "       cabar2 = plt.colorbar(sc2,  cax=cax2)\n",
        "       cabar2.set_label('Validation data, predicted noise')\n",
        "       cabar3 = plt.colorbar(sc3,  cax=cax3)\n",
        "       cabar3.set_label('Validation data, GT noise')\n",
        "       \n",
        "       \n",
        "\n",
        "\n",
        "       plot_path = os.path.join(result_path + '/FigureToPaper_%s.pdf' % (str(xlabel)+'_'+str(ylabel)))\n",
        "       plt.savefig(plot_path, format=\"pdf\", bbox_inches=\"tight\")\n",
        "       \n",
        "       plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "LWCAo9rA44Oz",
        "outputId": "e9488bff-1d56-44f5-ca35-cd4b0cc87774"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "# Import machine learning regression models\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.svm import SVR\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from statsmodels.nonparametric.kernel_regression import KernelReg\n",
        "from sklearn.metrics import r2_score\n",
        "\n",
        "# Import cross validation\n",
        "from sklearn.model_selection import KFold\n",
        "\n",
        "# Import linear algebra error\n",
        "from numpy.linalg import LinAlgError\n",
        "\n",
        "# Define the input features that will be considered. Each input feature is defined\n",
        "# by a tuple that indicates the transform, the probability distribution divergence\n",
        "# measure, and the digit (first or second)\n",
        "InputFeatures=[(\"FFT\",\"BD\",\"First\"),(\"DCT\",\"BD\",\"First\"),(\"DST\",\"BD\",\"First\"),\n",
        "               (\"FFT\",\"KL\",\"First\"),(\"DCT\",\"KL\",\"First\"),(\"DST\",\"KL\",\"First\"),\n",
        "               (\"FFT\",\"TV\",\"First\"),(\"DCT\",\"TV\",\"First\"),(\"DST\",\"TV\",\"First\"),\n",
        "               (\"FFT\",\"H\",\"First\"),(\"DCT\",\"H\",\"First\"),(\"DST\",\"H\",\"First\"),\n",
        "               (\"FFT\",\"JS\",\"First\"),(\"DCT\",\"JS\",\"First\"),(\"DST\",\"JS\",\"First\"),\n",
        "               (\"FFT\",\"BD\",\"Second\"),(\"DCT\",\"BD\",\"Second\"),(\"DST\",\"BD\",\"Second\"),\n",
        "               (\"FFT\",\"KL\",\"Second\"),(\"DCT\",\"KL\",\"Second\"),(\"DST\",\"KL\",\"Second\"),\n",
        "               (\"FFT\",\"TV\",\"Second\"),(\"DCT\",\"TV\",\"Second\"),(\"DST\",\"TV\",\"Second\"),\n",
        "               (\"FFT\",\"H\",\"Second\"),(\"DCT\",\"H\",\"Second\"),(\"DST\",\"H\",\"Second\"),\n",
        "               (\"FFT\",\"JS\",\"Second\"),(\"DCT\",\"JS\",\"Second\"),(\"DST\",\"JS\",\"Second\")\n",
        "               ]\n",
        "\n",
        "# Define the sets of model input features to be tried. Each set is defined by \n",
        "# a list of input features which are indices into the InputFeatures list.\n",
        "# The sets [0] and [3] were presented in the MICCAI paper. All the others are new.\n",
        "CountInputFeatures=15\n",
        "SetsInputFeatures=[]\n",
        "for NdxFeature in range(0,CountInputFeatures):\n",
        "  SetsInputFeatures.append([NdxFeature])\n",
        "for NdxFeature in range(0,CountInputFeatures):\n",
        "  for NdxFeature2 in range(NdxFeature+1,CountInputFeatures):\n",
        "    SetsInputFeatures.append([NdxFeature,NdxFeature2])\n",
        "\n",
        "# These additional sets of features perform fairly well\n",
        "#SetsInputFeatures.append([3,10,13])\n",
        "#SetsInputFeatures.append([3,4,5])\n",
        "#SetsInputFeatures.append([3,4,5,8])\n",
        "\n",
        "# The number of sets of inputs features, to be used afterwards\n",
        "NumSets=len(SetsInputFeatures)\n",
        "\n",
        "# Create a name for each set of input features. Each name is a string with the input\n",
        "# feature indices separated by blank spaces\n",
        "SetNames=[]\n",
        "for NdxSet in range(0,NumSets):\n",
        "  SetNames.append(\" \".join(map(str,SetsInputFeatures[NdxSet])))\n",
        "\n",
        "# Create the cross validation class to obtain the image indices for the training\n",
        "# and validation sets\n",
        "CrossValidator = KFold(n_splits=NumSplits, shuffle=True, random_state=1)\n",
        "\n",
        "# Prepare the validation results output variable\n",
        "ValidationResults={}\n",
        "\n",
        "for NdxFold, (TrainImagesIndices, ValidationImagesIndices) in enumerate(CrossValidator.split(np.arange(NumNoiselessImages))):\n",
        "  print(f\"Fold {NdxFold}:\")\n",
        "  print(f\"  Train: index={TrainImagesIndices}\")\n",
        "  print(f\"  Test:  index={ValidationImagesIndices}\")\n",
        "\n",
        "  # Obtain the training noise levels (outputs to be predicted by the regressors) from\n",
        "  # the training noiseless images \n",
        "  TrainingNoiseLevels=NoiseLevels[TrainImagesIndices,:]\n",
        "  # Reshape to the (n_samples, n_targets) format required by\n",
        "  # the sklearn.linear_model.LinearRegression class\n",
        "  TrainingNoiseLevelsFlat=np.reshape(TrainingNoiseLevels,(len(TrainImagesIndices)*NumNoiseLevels,1))\n",
        "\n",
        "  # Obtain the validation noise levels (outputs to be predicted by the regressors) from\n",
        "  # the validation noiseless images\n",
        "  ValidationNoiseLevels=NoiseLevels[ValidationImagesIndices,:]\n",
        "  # Reshape to the (n_samples, n_targets) format required by\n",
        "  # the sklearn.linear_model.LinearRegression class\n",
        "  ValidationNoiseLevelsFlat=np.reshape(ValidationNoiseLevels,(len(ValidationImagesIndices)*NumNoiseLevels,1))\n",
        "\n",
        "  \n",
        "  \n",
        "  # Train the models and validate their performance\n",
        "  NumSets=1 #Defined to do only one loop\n",
        "  for NdxSet in range(0,NumSets):\n",
        "    NdxSet=23 #Defined to choose couples of metrics with the best results. 12HLON=23; 21MMRR=15; 22NKI-RS=55; 21NKI-TRT=83; 20OASIS-TRT=67\n",
        "    # Print the variable indices for the current model\n",
        "    print(str(SetsInputFeatures[NdxSet][0]))\n",
        "    print(SetsInputFeatures[NdxSet][0])\n",
        "    print(SetsInputFeatures[NdxSet][1])\n",
        "    #break\n",
        "    \n",
        "    # Obtain the number of input features (distribution distances) for the current model\n",
        "    NumInputFeatures=len(SetsInputFeatures[NdxSet])\n",
        "    \n",
        "    # Obtain the input features (distribution distances) for the training and validation sets\n",
        "    TrainingDistributionDistances=np.zeros((NumInputFeatures,len(TrainImagesIndices),NumNoiseLevels))\n",
        "    ValidationDistributionDistances=np.zeros((NumInputFeatures,len(ValidationImagesIndices),NumNoiseLevels))\n",
        "    # Loop for all input features in the current set\n",
        "    for NdxInputFeature in range(0,NumInputFeatures):\n",
        "      # Fetch the input features for all noiseless images except the validation noiseless image\n",
        "      TrainingDistributionDistances[NdxInputFeature,:,:]=Divergences[InputFeatures[SetsInputFeatures[NdxSet][NdxInputFeature]]][TrainImagesIndices,:]\n",
        "      # Fetch the input features for the validation noiseless image\n",
        "      ValidationDistributionDistances[NdxInputFeature,:,:]=Divergences[InputFeatures[SetsInputFeatures[NdxSet][NdxInputFeature]]][ValidationImagesIndices,:]\n",
        "\n",
        "    # Reshape the input features to the format (n_samples, n_features) which is required\n",
        "    # by the sklearn.linear_model.LinearRegression class\n",
        "    TrainingDistributionDistancesFlat=np.reshape(TrainingDistributionDistances,(NumInputFeatures,len(TrainImagesIndices)*NumNoiseLevels)).transpose()\n",
        "    ValidationDistributionDistancesFlat=np.reshape(ValidationDistributionDistances,(NumInputFeatures,len(ValidationImagesIndices)*NumNoiseLevels)).transpose()\n",
        "\n",
        "    # Train the linear regression model\n",
        "    LinearModel1D=LinearRegression().fit(TrainingDistributionDistancesFlat,TrainingNoiseLevelsFlat)\n",
        "\n",
        "    # Train the polynomial regression model\n",
        "    poly = PolynomialFeatures(degree=2, include_bias=False)\n",
        "    TrainingPolyFeatures = poly.fit_transform(TrainingDistributionDistancesFlat)\n",
        "    PolyModel1D = LinearRegression()\n",
        "    PolyModel1D.fit(TrainingPolyFeatures, TrainingNoiseLevelsFlat)\n",
        "\n",
        "    # Train the random forest regression model\n",
        "    RandomForestModel=RandomForestRegressor(min_samples_leaf=20).fit(TrainingDistributionDistancesFlat,TrainingNoiseLevelsFlat.ravel())\n",
        "\n",
        "    # Train the support vector regression model\n",
        "    SVRModel = make_pipeline(StandardScaler(), SVR(C=1.0, epsilon=0.001)).fit(TrainingDistributionDistancesFlat,TrainingNoiseLevelsFlat.ravel())\n",
        "\n",
        "    # Train the kernel regression model\n",
        "    try:\n",
        "      KRModel = KernelReg(endog=TrainingNoiseLevelsFlat.ravel(), exog=TrainingDistributionDistancesFlat, var_type=\"c\"*NumInputFeatures, bw=\"cv_ls\") \n",
        "    except LinAlgError:\n",
        "      print(\"Linear algebra error while training the kernel regressor\")\n",
        "\n",
        "    # Validate the linear regression model\n",
        "    ValidationNoiseLevelsLinear=LinearModel1D.predict(ValidationDistributionDistancesFlat)\n",
        "    ValidationResults[NdxFold,NdxSet,\"MSE\",\"Linear\"]=np.mean(np.power(ValidationNoiseLevelsLinear-ValidationNoiseLevelsFlat,2))\n",
        "    ValidationResults[NdxFold,NdxSet,\"MAE\",\"Linear\"]=np.mean(np.absolute(ValidationNoiseLevelsLinear-ValidationNoiseLevelsFlat))\n",
        "    ValidationResults[NdxFold,NdxSet,\"R2\",\"Linear\"]=r2_score(ValidationNoiseLevelsFlat,ValidationNoiseLevelsLinear)\n",
        "\n",
        "    # Validate the polynomial regression model\n",
        "    ValidationPolyFeatures = poly.fit_transform(ValidationDistributionDistancesFlat)\n",
        "    ValidationNoiseLevelsPoly=PolyModel1D.predict(ValidationPolyFeatures)\n",
        "    ValidationResults[NdxFold,NdxSet,\"MSE\",\"Poly\"]=np.mean(np.power(ValidationNoiseLevelsPoly-ValidationNoiseLevelsFlat,2))\n",
        "    ValidationResults[NdxFold,NdxSet,\"MAE\",\"Poly\"]=np.mean(np.absolute(ValidationNoiseLevelsPoly-ValidationNoiseLevelsFlat))\n",
        "    ValidationResults[NdxFold,NdxSet,\"R2\",\"Poly\"]=r2_score(ValidationNoiseLevelsFlat,ValidationNoiseLevelsPoly)  \n",
        "\n",
        "    # Validate the random forest regression model\n",
        "    ValidationNoiseLevelsRandomForest=np.reshape(RandomForestModel.predict(ValidationDistributionDistancesFlat),(NumNoiseLevels*len(ValidationImagesIndices),1))\n",
        "    ValidationResults[NdxFold,NdxSet,\"MSE\",\"RandomForest\"]=np.mean(np.power(ValidationNoiseLevelsRandomForest-ValidationNoiseLevelsFlat,2))\n",
        "    ValidationResults[NdxFold,NdxSet,\"MAE\",\"RandomForest\"]=np.mean(np.absolute(ValidationNoiseLevelsRandomForest-ValidationNoiseLevelsFlat))\n",
        "    ValidationResults[NdxFold,NdxSet,\"R2\",\"RandomForest\"]=r2_score(ValidationNoiseLevelsFlat,ValidationNoiseLevelsRandomForest)  \n",
        "    \n",
        "    # Validate the support vector regression model\n",
        "    ValidationNoiseLevelsSVR=np.reshape(SVRModel.predict(ValidationDistributionDistancesFlat),(NumNoiseLevels*len(ValidationImagesIndices),1))\n",
        "    ValidationResults[NdxFold,NdxSet,\"MSE\",\"SVR\"]=np.mean(np.power(ValidationNoiseLevelsSVR-ValidationNoiseLevelsFlat,2))\n",
        "    ValidationResults[NdxFold,NdxSet,\"MAE\",\"SVR\"]=np.mean(np.absolute(ValidationNoiseLevelsSVR-ValidationNoiseLevelsFlat))\n",
        "    ValidationResults[NdxFold,NdxSet,\"R2\",\"SVR\"]=r2_score(ValidationNoiseLevelsFlat,ValidationNoiseLevelsSVR)  \n",
        "\n",
        "    # Validate the kernel regression model\n",
        "    # Error to estimate hyperparameter\n",
        "    try:\n",
        "      (Averages,StandardDeviations)=KRModel.fit(ValidationDistributionDistancesFlat)\n",
        "      ValidationNoiseLevelsKR=np.reshape(Averages,(NumNoiseLevels*len(ValidationImagesIndices),1))\n",
        "      ValidationResults[NdxFold,NdxSet,\"R2\",\"KernelRegression\"]=r2_score(ValidationNoiseLevelsFlat,ValidationNoiseLevelsKR)  \n",
        "      ValidationResults[NdxFold,NdxSet,\"MSE\",\"KernelRegression\"]=np.mean(np.power(ValidationNoiseLevelsKR-ValidationNoiseLevelsFlat,2))\n",
        "      ValidationResults[NdxFold,NdxSet,\"MAE\",\"KernelRegression\"]=np.mean(np.absolute(ValidationNoiseLevelsKR-ValidationNoiseLevelsFlat))\n",
        "    except LinAlgError: # Error del 21MMRR, fallo en entrenamiento, continua y hae el fit. Pero falla en reshape.\n",
        "      print(\"Linear algebra error while training the kernel regressor so can not do reshape after fit.\")\n",
        "      ValidationResults[NdxFold,NdxSet,\"R2\",\"KernelRegression\"]=r2_score(ValidationNoiseLevelsFlat,ValidationNoiseLevelsKR)   \n",
        "      ValidationResults[NdxFold,NdxSet,\"MSE\",\"KernelRegression\"]=np.mean(np.power(ValidationNoiseLevelsKR-ValidationNoiseLevelsFlat,2))\n",
        "      ValidationResults[NdxFold,NdxSet,\"MAE\",\"KernelRegression\"]=np.mean(np.absolute(ValidationNoiseLevelsKR-ValidationNoiseLevelsFlat))\n",
        "    except ValueError: # Error del 22NKI, Nan en Average y da error en R2.\n",
        "      print('Nan or Inf value in R2 because of to KernelModel small bandwith')\n",
        "      ValidationResults[NdxFold,NdxSet,\"MSE\",\"KernelRegression\"]=np.Inf\n",
        "      ValidationResults[NdxFold,NdxSet,\"MAE\",\"KernelRegression\"]=np.Inf\n",
        "      ValidationResults[NdxFold,NdxSet,\"R2\",\"KernelRegression\"]=0\n",
        "    \n",
        "      \n",
        "    # Plot the best variables set (choose by first row in excel file), (two metrics) and the best model (you must choose best model manually actualizing predictedValuesOfBestModel variable)\n",
        "  \n",
        "    \n",
        "    # Best model, choose one of the following:\n",
        "    #ValidationNoiseLevelsLinear, \n",
        "    #ValidationNoiseLevelsPoly,\n",
        "    #ValidationNoiseLevelsFlat,\n",
        "    #ValidationNoiseLevelsRandomForest,\n",
        "    #ValidationNoiseLevelsSVR,\n",
        "    #ValidationNoiseLevelsKR\n",
        "    predictedValuesOfBestModel = ValidationNoiseLevelsKR # Change this in each repository\n",
        "\n",
        "    plot_3D(xValidation=ValidationDistributionDistancesFlat[:,0], \n",
        "    yValidation=ValidationDistributionDistancesFlat[:,1], \n",
        "    noiseValidationPredicted=predictedValuesOfBestModel,\n",
        "    noiseValidationGT=ValidationNoiseLevelsFlat,\n",
        "    xTraining = TrainingDistributionDistancesFlat[:,0],\n",
        "    yTraining = TrainingDistributionDistancesFlat[:,1],\n",
        "    noiseTrainingGT = TrainingNoiseLevelsFlat,\n",
        "    xlabel = str(SetsInputFeatures[NdxSet][0]+1),\n",
        "    ylabel = str(SetsInputFeatures[NdxSet][1]+1)) \n",
        "  \n",
        "  break # Stopping the loop and plotting only one figure."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "benford040",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    },
    "vscode": {
      "interpreter": {
        "hash": "2e168a325c0c280b551173248b6d4b0c85882bf0215c04a8e6edfcd8716a8cb3"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
